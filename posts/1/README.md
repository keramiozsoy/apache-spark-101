# Post 1

## What is Apache Spark

Apache Spark is, unified analytics engine for big data, a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.


Apache Spark is used variety of software developers. It has been written with Scala Programming Language.  But it is not mandatory to use Scala. 
There are another APIs for software developers.

- Scala (https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html)

- .NET  ( https://github.com/dotnet/spark )

- Java (https://spark.apache.org/docs/latest/api/java/index.html)

- Python ( https://spark.apache.org/docs/latest/api/python/index.html)

- R ( https://spark.apache.org/docs/latest/api/R/index.html )

- SQL ( https://spark.apache.org/docs/latest/api/sql/index.html ) 


Apache Spark is a framework that enables us to perform parallel operations on big data sets.


There is also Apache Hadoop, another project that provides parallel processing of large data sets. Apache Hadoop uses the map-reduce model for itself.